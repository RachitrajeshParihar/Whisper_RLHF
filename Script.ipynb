{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Install dependencies\n",
    "!pip install gradio openai-whisper torch pydub\n",
    "!apt-get install ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Final version\n",
    "import os\n",
    "import whisper\n",
    "from pydub import AudioSegment\n",
    "from difflib import SequenceMatcher\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "# Step 2: Load the Whisper model\n",
    "try:\n",
    "    model = whisper.load_model(\"base\")\n",
    "    print(\"Whisper model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Whisper model: {e}\")\n",
    "\n",
    "# Step 3: Audio transcription function\n",
    "def transcribe_audio(file_path):\n",
    "    \"\"\"\n",
    "    Converts the input file to .wav format if needed and transcribes it using Whisper.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"File not found: {file_path}\"\n",
    "\n",
    "        # Convert audio to WAV and resample to 16 kHz\n",
    "        audio = AudioSegment.from_file(file_path)\n",
    "        audio = audio.set_frame_rate(16000)  # Resample\n",
    "        wav_file_path = file_path + \".wav\"\n",
    "        audio.export(wav_file_path, format=\"wav\")  # Export as .wav\n",
    "\n",
    "        # Transcribe the audio\n",
    "        result = model.transcribe(wav_file_path)\n",
    "        return result['text']\n",
    "    except Exception as e:\n",
    "        return f\"Error processing audio: {e}\"\n",
    "\n",
    "# Step 4: Feedback collection function\n",
    "def collect_feedback(transcription, human_transcription):\n",
    "    \"\"\"\n",
    "    Generates feedback comparing Whisper transcription with human transcription.\n",
    "    \"\"\"\n",
    "    diff = SequenceMatcher(None, transcription, human_transcription).get_opcodes()\n",
    "    feedback = []\n",
    "    for tag, i1, i2, j1, j2 in diff:\n",
    "        if tag == \"replace\":\n",
    "            feedback.append(f\"Replaced '{transcription[i1:i2]}' with '{human_transcription[j1:j2]}'\")\n",
    "        elif tag == \"delete\":\n",
    "            feedback.append(f\"Deleted '{transcription[i1:i2]}'\")\n",
    "        elif tag == \"insert\":\n",
    "            feedback.append(f\"Inserted '{human_transcription[j1:j2]}'\")\n",
    "    return \"\\n\".join(feedback)\n",
    "\n",
    "# Step 5: Reward calculation function\n",
    "def calculate_reward(transcription, human_transcription):\n",
    "    \"\"\"\n",
    "    Calculates similarity reward based on shared words between transcription and human transcription.\n",
    "    \"\"\"\n",
    "    transcription_words = set(transcription.split())\n",
    "    human_words = set(human_transcription.split())\n",
    "    shared_words = transcription_words & human_words\n",
    "    return len(shared_words) / max(len(human_words), 1)\n",
    "\n",
    "# Step 6: Model saving function\n",
    "def save_model(output_dir):\n",
    "    \"\"\"\n",
    "    Saves the Whisper model's configuration and weights to the specified directory.\n",
    "    Since Whisper doesn't support direct fine-tuning, this demonstrates saving.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    try:\n",
    "        # Save model state (simulated)\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "        with open(os.path.join(output_dir, \"config.json\"), \"w\") as f:\n",
    "            f.write(model.config.to_json_string())\n",
    "        print(f\"Model state and configuration saved to {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "# Step 7: Gradio app\n",
    "def main():\n",
    "    with gr.Blocks() as app:\n",
    "        gr.Markdown(\"### Whisper RLHF Workflow\")\n",
    "\n",
    "        # Input fields\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload Audio File\")\n",
    "        transcription_output = gr.Textbox(label=\"Whisper Transcription\", interactive=False)\n",
    "        human_transcription = gr.Textbox(label=\"Human Transcription\")\n",
    "        feedback_output = gr.Textbox(label=\"Feedback\", interactive=False)\n",
    "        reward_output = gr.Number(label=\"Reward\", interactive=False)\n",
    "        status_output = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "        # State variables\n",
    "        current_transcription = gr.State()\n",
    "\n",
    "        # Step 1: Transcribe audio\n",
    "        def transcribe(file_path):\n",
    "            transcription = transcribe_audio(file_path)\n",
    "            return transcription, transcription\n",
    "\n",
    "        # Step 2: Process feedback and calculate reward\n",
    "        def process_feedback(transcription, human_transcription):\n",
    "            feedback = collect_feedback(transcription, human_transcription)\n",
    "            reward = calculate_reward(transcription, human_transcription)\n",
    "            save_model(\"/kaggle/working/updated_whisper\")  # Save updated state\n",
    "            return feedback, reward, \"Model state saved successfully.\"\n",
    "\n",
    "        # Define button interactions\n",
    "        transcribe_button = gr.Button(\"Transcribe Audio\")\n",
    "        transcribe_button.click(\n",
    "            transcribe,\n",
    "            inputs=[audio_input],\n",
    "            outputs=[transcription_output, current_transcription],\n",
    "        )\n",
    "\n",
    "        feedback_button = gr.Button(\"Submit Feedback\")\n",
    "        feedback_button.click(\n",
    "            process_feedback,\n",
    "            inputs=[current_transcription, human_transcription],\n",
    "            outputs=[feedback_output, reward_output, status_output],\n",
    "        )\n",
    "\n",
    "    app.launch()\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
